# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ app.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, json, textwrap, datetime, requests, bs4, pandas as pd
import streamlit as st
from newspaper import Article
from duckduckgo_search import DDGS
import openai, warnings

warnings.filterwarnings("ignore", category=SyntaxWarning)
openai.api_key = st.secrets["OPENAI_API_KEY"]

st.set_page_config(page_title="SQEG ã‚¯ã‚¤ãƒƒã‚¯ãƒã‚§ãƒƒã‚«ãƒ¼", page_icon="ğŸ”")
st.title("ğŸ” SQEG 2025-01 ç°¡æ˜“è©•ä¾¡ãƒ„ãƒ¼ãƒ«")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æœ¬æ–‡å–å¾— (URL / ç›´æ¥å…¥åŠ›) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch(src: str):
    """URLãªã‚‰ newspaper3k â†’ å¤±æ•—æ™‚ requests+BS4ã€‚æœ¬æ–‡ãŒå–ã‚Œãªã‘ã‚Œã°ç©ºæ–‡å­—."""
    if src.startswith("http"):
        try:
            art = Article(src, language="ja")
            art.download(); art.parse()
            return art.title or "", art.text
        except Exception:
            try:
                html = requests.get(src, timeout=10, headers={"User-Agent": "Mozilla/5.0"}).text
                soup = bs4.BeautifulSoup(html, "html.parser")
                title = soup.title.string.strip() if soup.title else ""
                body  = "\n".join(p.get_text(" ", strip=True) for p in soup.find_all("p"))
                return title, body[:20000]
            except Exception:
                return "", ""
    return "", src  # ç›´æ¥æœ¬æ–‡

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ é¡ä¼¼ãƒšãƒ¼ã‚¸å€™è£œ (ã‚³ãƒ”ãƒ¼ç–‘æƒ‘ãƒã‚§ãƒƒã‚¯) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_ddg(query: str, k: int = 5):
    out = []
    with DDGS() as ddgs:
        for r in ddgs.text(query, max_results=k):
            out.append(f"{r['title']} â€” {r['body']}")
    return out

PROMPT = """
ã‚ãªãŸã¯ Google Search Quality Evaluator ã§ã™ã€‚
è¨˜äº‹æœ¬æ–‡ã¨é¡ä¼¼ãƒšãƒ¼ã‚¸å€™è£œã‚’æ¸¡ã—ã¾ã™ã€‚
SQEG 2025-01 (3.2 / 4.6.6 / 5.2.1 / 7.1) ã«å¾“ã„æ¬¡ã®JSONã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
{
  "pq":"Lowest|Low|Medium|High|Highest",
  "nm":"Fails|Slightly|Moderately|Highly|Fully",
  "effort":0-5,"originality":0-5,"duplication_rate":0-100,
  "skill":0-5,"accuracy":0-5,
  "eeat_summary":"<100å­—ä»¥å†…>","improvement_advice":"<200å­—ä»¥å†…>"
}
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
src = st.text_area("URL ã¾ãŸã¯æœ¬æ–‡ã‚’å…¥åŠ›ã—ã¦ã€è©•ä¾¡ã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„", height=200)
if st.button("è©•ä¾¡ã™ã‚‹") and src.strip():
    with st.spinner("è©•ä¾¡ä¸­â€¦"):
        title, body = fetch(src.strip())
        if not body:
            st.error("âŒ è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\nURL ã§ãªãæœ¬æ–‡ã‚’ç›´æ¥è²¼ã£ã¦è©¦ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        query = title or " ".join(body.split()[:15])
        try:
            chat = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                temperature=0.1,
                messages=[
                    {"role": "system", "content": PROMPT},
                    {"role": "user", "content": f"###æœ¬æ–‡\n{textwrap.shorten(body, width=12000, placeholder="...[cut]...")}\n\n###é¡ä¼¼\n" + "\n".join(search_ddg(query))}]
            )
            result = json.loads(chat.choices[0].message.content)
        except Exception as e:
            st.error("âŒ OpenAI ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãŒè¿”ã‚Šã¾ã—ãŸï¼ˆãƒ¢ãƒ‡ãƒ«æ¨©é™ãƒ»ã‚­ãƒ¼æ®‹é«˜ã‚’ã”ç¢ºèªãã ã•ã„ï¼‰")
            st.exception(e)
            st.stop()

    # çµæœè¡¨ç¤º
    st.subheader("è©•ä¾¡çµæœ")
    st.json(result, expanded=False)
    if result["pq"] in ["Lowest", "Low"]:
        st.error("âš ï¸ PQ ãŒä½è©•ä¾¡ã§ã™ã€‚ãƒªãƒ©ã‚¤ãƒˆã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

    # CSV ãƒ­ã‚°
    pd.DataFrame([{
        "timestamp": datetime.datetime.now().isoformat(timespec="seconds"),
        "source": src[:100], **result
    }]).to_csv("sqeg_log.csv", mode="a", index=False, header=not os.path.exists("sqeg_log.csv"))
    st.success("çµæœã‚’ sqeg_log.csv ã«è¿½è¨˜ã—ã¾ã—ãŸ âœ…")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
